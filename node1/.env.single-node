# Optimized Single-Node Configuration for Agentic Coding
# Hardware: 2x NVIDIA L40 (96GB total)
# Model: Qwen3-30B-A3B-Instruct-2507 (MoE: 31B total, 3B active)

# ============================================
# REQUIRED: Set your HuggingFace token
# ============================================
HF_TOKEN=your_huggingface_token_here

# ============================================
# MODEL CONFIGURATION
# ============================================
# Qwen3 MoE model - 31B params, only 3B active = FAST inference
MODEL_NAME=Qwen/Qwen3-30B-A3B-Instruct-2507

# Alternative Qwen3 models:
# MODEL_NAME=Qwen/Qwen3-32B-Instruct           # Dense 32B (slower, fits in FP16)
# MODEL_NAME=Qwen/Qwen3-30B-A3B-Thinking-2507  # With thinking mode

# Model cache directory
MODEL_CACHE_DIR=./models

# ============================================
# CONTEXT & MEMORY CONFIGURATION
# ============================================
# Context length (native max: 262,144)
# 131K is a good balance for agentic workloads on 96GB
MAX_MODEL_LEN=131072

# For maximum context (262K), reduce MAX_NUM_SEQS:
# MAX_MODEL_LEN=262144
# MAX_NUM_SEQS=4

# Concurrent sequences (more = better throughput, more VRAM)
MAX_NUM_SEQS=16

# Max tokens per batch (controls prefill chunking)
MAX_BATCHED_TOKENS=32768

# ============================================
# SERVER CONFIGURATION
# ============================================
VLLM_PORT=8000

# ============================================
# PERFORMANCE TUNING
# ============================================
# GPU memory utilization (0.90 is safe, increase to 0.95 if stable)
GPU_MEMORY_UTILIZATION=0.90

# Attention backend (FLASHINFER is fastest for MoE)
VLLM_ATTENTION_BACKEND=FLASHINFER

# ============================================
# EXTENDED CONTEXT (1M tokens) - ADVANCED
# ============================================
# Requires DUAL_CHUNK_FLASH_ATTN and VLLM_USE_V1=0
# Not recommended for L40s - needs ~240GB VRAM
# VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN
# MAX_MODEL_LEN=1010000
