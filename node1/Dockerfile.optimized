# Optimized Dockerfile for vLLM with FlashInfer
# Optimized for: NVIDIA L40, Qwen3-30B-A3B, Agentic workloads

FROM vllm/vllm-openai:latest

USER root

# Ensure python symlink exists (vLLM image uses python3)
RUN ln -sf /usr/bin/python3 /usr/bin/python || true

# Install hf_transfer for faster model downloads
RUN pip install --no-cache-dir hf_transfer

# Create workspace
WORKDIR /workspace
