# Optimized Single-Node vLLM Configuration
# For: 2x NVIDIA L40 (96GB total) with GLM-4.6V-Flash
# Optimized for: Vision-Language with screenshot analysis

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile.optimized
    image: vllm-qwen3:local
    container_name: vllm-server
    hostname: vllm-server
    network_mode: host
    ipc: host
    restart: unless-stopped
    runtime: nvidia
    user: root
    environment:
      # GPU Configuration
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0,1

      # HuggingFace Configuration
      - HF_TOKEN=${HF_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=1

      # Network interface for distributed communication
      - GLOO_SOCKET_IFNAME=${NETWORK_INTERFACE:-ens33}
      - NCCL_SOCKET_IFNAME=${NETWORK_INTERFACE:-ens33}

      # NCCL Optimizations for multi-GPU
      - NCCL_SHM_DISABLE=0
      - NCCL_IB_DISABLE=1

    volumes:
      - ./models:/models
      - ./data:/data
      - ${MODEL_CACHE_DIR:-./models}:/root/.cache/huggingface
      # Mount directory for local media/images
      - ./images:/workspace/images:ro
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "============================================"
        echo "Starting vLLM server for GLM-4.6V-Flash"
        echo "Configuration: 2x L40 GPUs (Vision-Language)"
        echo "Model: ${MODEL_NAME}"
        echo "============================================"

        python3 -m vllm.entrypoints.openai.api_server \
          --host 0.0.0.0 \
          --port ${VLLM_PORT:-8000} \
          --model ${MODEL_NAME:-zai-org/GLM-4.6V-Flash} \
          --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-1} \
          --quantization ${QUANTIZATION:-fp8} \
          --max-model-len ${MAX_MODEL_LEN:-32768} \
          --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.90} \
          --max-num-seqs ${MAX_NUM_SEQS:-8} \
          --max-num-batched-tokens 32768 \
          --trust-remote-code \
          --disable-log-requests \
          --allowed-local-media-path /workspace/images \
          --mm-encoder-tp-mode data \
          --mm-processor-cache-type shm \
          --served-model-name glm-4.6v-flash
    shm_size: 16gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
