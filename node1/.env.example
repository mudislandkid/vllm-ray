# Hugging Face token for accessing gated models
HF_TOKEN=your_huggingface_token_here

# Model configuration
MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
MODEL_CACHE_DIR=./models

# vLLM server configuration
VLLM_PORT=8000
MAX_MODEL_LEN=4096

# Parallelism settings (total GPUs = TENSOR_PARALLEL_SIZE * PIPELINE_PARALLEL_SIZE)
TENSOR_PARALLEL_SIZE=2
PIPELINE_PARALLEL_SIZE=2

# Startup delay in seconds (wait for worker nodes to connect)
VLLM_STARTUP_DELAY=60

# Network interface for distributed communication (GLOO/NCCL)
# Find your interface with: ip addr show
NETWORK_INTERFACE=ens33
