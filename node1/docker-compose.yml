# Node 1 - Ray Head + vLLM (same image, same Ray version)

services:
  ray-head:
    build:
      context: .
      dockerfile: Dockerfile
    image: ray-vllm:local
    container_name: ray-head
    hostname: ray-head
    network_mode: host
    ipc: host
    restart: unless-stopped
    runtime: nvidia
    user: root
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ./models:/models
      - ./data:/data
      - ${MODEL_CACHE_DIR:-./models}:/root/.cache/huggingface
    command: >
      ray start --head
      --port=6379
      --dashboard-host=0.0.0.0
      --dashboard-port=8265
      --num-gpus=2
      --block
    shm_size: 10gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vllm-server:
    image: ray-vllm:local
    container_name: vllm-server
    hostname: vllm-server
    network_mode: host
    ipc: host
    restart: on-failure
    runtime: nvidia
    user: root
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
      - VLLM_HOST_IP=${VLLM_HOST_IP:-10.15.105.105}
      - GLOO_SOCKET_IFNAME=${NETWORK_INTERFACE:-ens33}
      - NCCL_SOCKET_IFNAME=${NETWORK_INTERFACE:-ens33}
    volumes:
      - ./models:/models
      - ./data:/data
      - ${MODEL_CACHE_DIR:-./models}:/root/.cache/huggingface
    depends_on:
      - ray-head
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "Waiting for Ray cluster to be ready..."
        sleep ${VLLM_STARTUP_DELAY:-60}

        echo "Ray cluster status:"
        ray status

        echo "Starting vLLM server..."
        python -m vllm.entrypoints.openai.api_server \
          --host 0.0.0.0 \
          --port ${VLLM_PORT:-8000} \
          --model ${MODEL_NAME:-meta-llama/Llama-3.1-70B-Instruct} \
          --tensor-parallel-size ${TENSOR_PARALLEL_SIZE:-2} \
          --pipeline-parallel-size ${PIPELINE_PARALLEL_SIZE:-2} \
          --distributed-executor-backend ray \
          --trust-remote-code \
          --dtype auto \
          --max-model-len ${MAX_MODEL_LEN:-4096}
    shm_size: 10gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
