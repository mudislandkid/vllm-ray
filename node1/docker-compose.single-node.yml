# Optimized Single-Node vLLM Configuration
# For: 2x NVIDIA L40 (96GB total) with Qwen3-VL-30B-A3B-Instruct
# Optimized for: Vision-Language agentic coding with tool calling

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile.optimized
    image: vllm-qwen3:local
    container_name: vllm-server
    hostname: vllm-server
    network_mode: host
    ipc: host
    restart: unless-stopped
    runtime: nvidia
    user: root
    environment:
      # GPU Configuration
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0,1

      # HuggingFace Configuration
      - HF_TOKEN=${HF_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=1

      # Network interface for distributed communication
      - GLOO_SOCKET_IFNAME=${NETWORK_INTERFACE:-ens33}
      - NCCL_SOCKET_IFNAME=${NETWORK_INTERFACE:-ens33}

      # NCCL Optimizations for multi-GPU
      - NCCL_SHM_DISABLE=0
      - NCCL_IB_DISABLE=1

    volumes:
      - ./models:/models
      - ./data:/data
      - ${MODEL_CACHE_DIR:-./models}:/root/.cache/huggingface
      # MoE tuned configs for L40
      - ./moe-configs:/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs:ro
      # MoE tuning script
      - ./tune-moe.sh:/workspace/tune-moe.sh:ro
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        echo "============================================"
        echo "Starting vLLM server for Qwen3-VL-30B-A3B"
        echo "Configuration: 2x L40 GPUs (Vision-Language)"
        echo "============================================"

        python3 -m vllm.entrypoints.openai.api_server \
          --host 0.0.0.0 \
          --port ${VLLM_PORT:-8000} \
          --model ${MODEL_NAME:-Qwen/Qwen3-VL-30B-A3B-Instruct} \
          --tensor-parallel-size 2 \
          --dtype bfloat16 \
          --max-model-len ${MAX_MODEL_LEN:-32768} \
          --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION:-0.90} \
          --max-num-seqs ${MAX_NUM_SEQS:-8} \
          --limit-mm-per-prompt '{"image": 4}' \
          --trust-remote-code \
          --disable-log-requests \
          --enable-auto-tool-choice \
          --tool-call-parser hermes \
          --max-num-batched-tokens 16384 \
          --served-model-name qwen3-vl
    shm_size: 16gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
