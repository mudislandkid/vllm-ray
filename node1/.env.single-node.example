# Optimized Single-Node Configuration for Vision-Language Agentic Coding
# Hardware: 2x NVIDIA L40 (96GB total)
# Model: Qwen3-VL-30B-A3B-Instruct (Vision-Language MoE)

# ============================================
# REQUIRED: Set your HuggingFace token
# ============================================
HF_TOKEN=your_huggingface_token_here

# ============================================
# MODEL CONFIGURATION
# ============================================
# Qwen3 Vision-Language MoE model
MODEL_NAME=Qwen/Qwen3-VL-30B-A3B-Instruct

# Alternative models:
# MODEL_NAME=Qwen/Qwen3-30B-A3B-Instruct-2507   # Text-only (faster, longer context)
# MODEL_NAME=Qwen/Qwen3-32B-Instruct            # Dense text model

# Model cache directory
MODEL_CACHE_DIR=./models

# ============================================
# CONTEXT & MEMORY CONFIGURATION
# ============================================
# Context length (VL models need more memory for image tokens)
# 32K is safe for VL with images, increase if text-only queries
MAX_MODEL_LEN=32768

# For longer context without images:
# MAX_MODEL_LEN=65536
# MAX_NUM_SEQS=4

# Concurrent sequences (VL needs fewer due to image memory)
MAX_NUM_SEQS=8

# ============================================
# SERVER CONFIGURATION
# ============================================
VLLM_PORT=8000

# ============================================
# PERFORMANCE TUNING
# ============================================
# GPU memory utilization
GPU_MEMORY_UTILIZATION=0.90
